# Embeddings training using Law Texts

## Steps

- Structure texts into dataset
    - TJ-SC
    - STJ 
    - STF

- Create another dataset with additional documents such as Vade Mecum

- Create several embeddings with:
    - Length:       50, 100, 500, 1000 
    - Algorithms:   Word2Vec, GLOVe, ELmo, Doc2Vec etc.

- Apply to machine learning algorithms like:
    - CNN
    
## References
- WANG, S., ZHOU, W., JIANG, C. "A survey of word embeddings based on deep learning", Computing, v. 27, n. 2, p. 171–198, 12 nov. 2019. DOI: 10.1007/s00607-019-00768-7. 
Disponível em: https://doi.org/10.1007/s00607-019-00768-7.

- DEVLIN, J., CHANG, M.-W., LEE, K., et al. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", p. 4171–4186, 10 out. 2018. 
Disponível em: http://arxiv.org/abs/1810.04805.

- PETERS, M., NEUMANN, M., IYYER, M., et al. "Deep Contextualized Word Representations". 2018. Anais [...] Stroudsburg, PA, USA, Association for Computational Linguistics, 2018. p. 2227–2237. DOI: 10.18653/v1/N18-1202. 
Disponível em: http://aclweb.org/anthology/N18-1202. 

- CHALKIDIS, I., KAMPAS, D. "Deep learning in law: early adaptation and legal word embeddings trained on large corpora", Artificial Intelligence and Law, v. 27, n. 2, p. 171–198, 2019. DOI: 10.1007/s10506-018-9238-9. 
Disponível em: https://doi.org/10.1007/s10506-018-9238-9.

- MIKOLOV, T., SUTSKEVER, I., CHEN, K., et al. "Distributed Representations of Words and Phrases and their Compositionality", Advances in Neural Information Processing Systems, p. 1–9, 16 out. 2013. 
Disponível em: http://arxiv.org/abs/1310.4546.

- BOJANOWSKI, P., GRAVE, E., JOULIN, A., et al. "Enriching Word Vectors with Subword Information", Transactions of the Association for Computational Linguistics, v. 5, p. 135–146, 15 jul. 2016. DOI: 10.1162/tacl_a_00051. 
Disponível em: http://arxiv.org/abs/1607.04606.

- PENNINGTON, J., SOCHER, R., MANNING, C. "Glove: Global Vectors for Word Representation". 2014. Anais [...] Stroudsburg, PA, USA, Association for Computational Linguistics, 2014. p. 1532–1543. DOI: 10.3115/v1/D14-1162. 
Disponível em: http://aclweb.org/anthology/D14-1162. 

- LAI, S., LIU, K., HE, S., et al. "How to Generate a Good Word Embedding", IEEE Intelligent Systems, v. 31, n. 6, p. 5–14, nov. 2016. DOI: 10.1109/MIS.2016.45. 
Disponível em: http://ieeexplore.ieee.org/document/7478417/.

- LINZEN, T. "Issues in evaluating semantic spaces using word analogies", n. Figure 2, 24 jun. 2016. 
Disponível em: http://arxiv.org/abs/1606.07736.

- HARTMANN, N., FONSECA, E., SHULBY, C., et al. "Portuguese Word Embeddings: Evaluating on Word Analogies and Natural Language Tasks", n. Section 3, 20 ago. 2017. 
Disponível em: http://arxiv.org/abs/1708.06025.

